---
title: "Walmart Sales Prediction"
author: "Sebastian Alvarado"
date: "August 14, 2016"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Introduction
Predicting future sales for a company is one of the most important aspects of strategic planning. We would like to analyze how internal and external factors of one of the biggest 
companies in the US can affect their Weekly Sales in the future. This model tries to achieve 
an approximate weekly sales prediction looking at previous years performance per Store on 
a weekly basis. 

The data collected ranges from 2010 to 2012, where 45 Walmart stores across the country were
included in this analysis. It is important to note that we also have external data available
like CPI,Unemployment Rate and Fuel Prices in the region of each store which, hopefully, help 
us to make a more detailed analysis. 

To learn more about Walmart and what is their business model you can read about the company [here](https://en.wikipedia.org/wiki/Walmart).

#Data Explanation
We had access to four different data sets from Kaggle.com about the company. These data sets
contained information about the stores,departments,temperature, unemployment etc. We will explain each one of the data sets in more detail with each one of its features.  
**Stores**:  
  - Store: The store number. Range from 1-45.  
  - Type: Three types of stores 'A', 'B' or 'C'.  
  - Size: Sets the size of a Store  would be calculated by the no. of products available in the     particular store ranging from 34,000 to 210,000.    
**Train**:  
  -Date: The date of the week where this observation was taken . 
  -Weekly_Sales: The sales recorded during that Week.  
  -Store: The store which observation in recorded 1-45.   
  -Dept: One of 1-99 that shows the department.  
  -IsHoliday: Boolean value representing a holiday week or not.  
**Features**: 
  -Temperature:Temperature of the region during that week.  
  -Fuel_Price: Fuel Price in that region during that week.  
  -MarkDown1:5 : Represents the Type of markdown and what quantity was available during that week.  
  -CPI: Consumer Price Index during that week.  
  -Unemployment: The unemployment rate during that week in the region of the store.  


The test data have the same fields as the Train data, only the Weekly_Sales are empty.  
#Data Origin and Collection
```{r loading, results="hide"}
#Walmart Predictive Analysis
#Load Required Libraries

library(dplyr)
library(ggplot2)
library(reshape2)
library(readr)
library(lubridate)
library(rpart)
library(rattle)
library(car)
library(caret)
library(corrplot)
library(rpart.plot)
```
Ensure that you have train.csv, test.csv, stores.csv and features.csv in your current
working directory.  
```{r}
#Loading files to work with
train<- read_csv("train.csv")
stores <- read_csv("stores.csv")
features <- read_csv("features.csv")
```
Our first step will be to join our two tables by Store which is the common column.  
```{r}
stores$Store <- factor(stores$Store)
train$Store <- factor(train$Store)
train <- full_join(train,stores,by=c("Store"))
```
#Preparation
In this step of the process we will conduct some feature engineering, we will use the
features that our data currently has but will tweak them in a way that makes our analysis easier. The most important objective in this step is to generate new features that will help us produce a better model 
**Include a Week Number of the year**
```{r}
train$WeekNum <- as.numeric(format(train$Date+3,"%U"))
```
We have also noticed that some Weekly Sales contain negative values, after analyzing the data we have concluded that those refer to Returned Products from previous weeks.
**Add a Returns Column**
```{r}
train$Returns <- lapply(train$Weekly_Sales,function(sales){
  ifelse(sales < 0,sales,0)
})
train$Weekly_Sales <- lapply(train$Weekly_Sales,function(sales){
  ifelse(sales > 0,sales,0)
})
```

At the moment, our data frame contains 421570 observations, since the objective of this model is to predict the Weekly Sales of a particular store given previous years, external information and tendency we will add the sales per department and put it together into one observation. In other words we will not subdivide sales by department. Thus we can make our Weekly Sales to be our Net Sales since we now can do Weekly_Sales - Returns to avoid negative values.   
##Aggregating Weekly Sales to Net Sales
```{r, eval=FALSE}
final_data <- data.frame(Store=factor(),Date=as.Date(character()),Weekly_Sales=numeric(),IsHoliday=logical(),Type=factor(),WeekNum=factor())
aggregate_sales <- function(){
for(i in 1:45){
  store_data <- train %>% filter(Store == i)
  dates <- unique(train$Date)
  for(next_date in seq_along(dates)){
    current_date <- unique(train$Date)[[next_date]]
    date_data <- store_data %>% filter(Date==current_date)
    #Add all the weekly sales
    net_sales <- unlist(sum(date_data$Weekly_Sales)) - unlist(sum(date_data$Returns))
    #Construct the data frame and append it
    next_row <- data.frame(Store=i,Date=current_date,Weekly_Sales=net_sales,IsHoliday=date_data$IsHoliday[[1]],Type=date_data$Type[[1]],WeekNum=date_data$WeekNum)
    next_row$Store <- factor(next_row$Store)
    final_data <- rbind(final_data,next_row)
  }
}
  return(final_data)
}
# Sum the sales by store without taking into account each department
final_data <- aggregate_sales()
```

```{r, echo=FALSE}
train <- read_csv("merged_train.csv")
train$Weekly_Sales <- as.numeric(train$Weekly_Sales)
train$Store <- factor(train$Store)
train$Type <- factor(train$Type)
```

After performing this procedure we now have 6435 observations which makes our data more manageable for further analysis.  
```{r}
features$Store <- factor(features$Store)
#Merge our final_data with our features
train <- left_join(train,features,by=c("Store","Date","IsHoliday"))
# Make the NA markdown as 0
train$MarkDown1 <- sapply(train$MarkDown1, function(value){
  ifelse(is.na(value),0,value)
})
train$MarkDown2 <- sapply(train$MarkDown2, function(value){
  ifelse(is.na(value),0,value)
})
train$MarkDown3 <- sapply(train$MarkDown3, function(value){
  ifelse(is.na(value),0,value)
})
train$MarkDown4 <- sapply(train$MarkDown4, function(value){
  ifelse(is.na(value),0,value)
})
train$MarkDown5 <- sapply(train$MarkDown5, function(value){
  ifelse(is.na(value),0,value)
})
```


**Rank**
We will also add a feature called rank, which is getting the range of values of Weekly Sales.
We will make five Range Buckets namely A,B,C,D and E. We will also try to predict in which of this buckets a given store would lie in a given week. 
```{r,echo=FALSE}
#Range Weekly Sales: Divide our sales into five different groups
range_sales <- range(train$Weekly_Sales)
range <- (range_sales[[2]] - range_sales[[1]]) / 5
first <- c(range_sales[[1]], range_sales[[1]] + range)
second <- c(range_sales[[1]] + range, range_sales[[1]] + 2*range)
third <-c(range_sales[[1]] + 2*range, range_sales[[1]] + 3*range)
fourth <- c(range_sales[[1]] + 3*range, range_sales[[1]] + 4*range)
fifth <- c(range_sales[[1]] + 4*range, range_sales[[2]])
train$Rank <- sapply(train$Weekly_Sales, function(sales){
  if(sales >= first[[1]] &  sales <= first[[2]]){
    return('A')
  }
  else if(sales >= second[[1]] & sales <= second[[2]]){
    return('B')
  }
  else if(sales >= third[[1]] & sales <= third[[2]]){
    return('C')
  }
  else if(sales >= fourth[[1]] & sales <= fourth[[2]]){
    return('D')
  }
  else{
    return('E')
  }
})
train$Rank <- factor(train$Rank)
```
We will partition the training set into two different data frames in order to keep
our analysis consistent and avoid testing on our training data.

```{r}
#Subset our data into train and test
index <- createDataPartition(train$Weekly_Sales,list = FALSE,p=0.8)
train.train <-train[index,]
train.test <- train[-index,]
```
#Explortory Analysis
###Data Review  
For a small glimpse of how our data looks like we can refer to the following picture.  
```{r}
head(train.train)
```
For our exploration analysis we started with the aggregate() function because we wanted to 
know which Store and Type of store was having the most sales, on average. 
```{r,Aggregate}
aggregate(train.train[,"Weekly_Sales"], by=train.train[,c("Store"), drop=FALSE], mean)
aggregate(train.train[,"Weekly_Sales"], by=train.train[,c("Type"), drop=FALSE], mean)
aggregate(train.train[,"Weekly_Sales"], by=train.train[,c("Type"), drop=FALSE], max)
```

With this initial information, we wanted to dig a little deeper and that is why we decided that graphic models will help us to find the interaction between each of the variables with Weekly Sales. Our goal with this exploration was to find correlation, patterns or any other insight that revealed more information between diving into our predictive model. 

```{r,echo=FALSE}
ggplot(train.train,aes(x=CPI,y=Weekly_Sales)) + geom_point(aes(color=train.train$Type)) + geom_smooth() + scale_x_continuous(name="Consumer Price Index") + scale_y_continuous(name="Weekly Sales") + scale_color_discrete(name="Type")
ggplot(train.train,aes(x=Unemployment,y=Weekly_Sales)) + geom_point(aes(color=train.train$Type)) + geom_smooth()  + scale_x_continuous(name="Unemployment") + scale_y_continuous(name="Weekly Sales") + scale_color_discrete(name="Type")
ggplot(train.train,aes(x=Temperature,y=Weekly_Sales)) + geom_point(aes(color=train.train$Type)) + geom_smooth()  + scale_x_continuous(name="Temperature") + scale_y_continuous(name="Weekly Sales") + scale_color_discrete(name="Type")
```
  

In the previous graphs one can see that there is no clear correlation between Weekly Sales and Unemployment or Temperature. A clearer correlation is visible between CPI and Weekly Sales.
However what is clear from this analysis is that Type A stores have more sales than any other type.  

We also want to analyze what is the effect of the MarkDowns on the weekly sales of the company
after analyzing the graphs we decided to show the one that had more impact. However, as one can see the MarkDowns don't show an immense correlation with Sales.
```{r, echo=FALSE}
ggplot(train.train,aes(x=MarkDown4,y=Weekly_Sales)) + geom_point(aes(color=train.train$Type)) + geom_smooth()  + scale_x_continuous(name="MarkDown4") + scale_y_continuous(name="Weekly Sales") + scale_color_discrete(name="Type")

```


Plot Store sales divided by Type of Store, in the following plot we selected Type A stores
```{r}
A_stores <- train.train %>% filter(Type=='A')
ggplot(A_stores,aes(x=CPI,y=Weekly_Sales)) + geom_point(aes(color=A_stores$IsHoliday)) + geom_smooth()#Sales vary depending on the weeknum we are in
```
Now we want to partition the Weekly Sales based on a store, from our analysis we saw that Store 20 is the one with the most sales. We will analyze this stores' results
```{r,eval=FALSE}
store_graph <- train.train %>% filter(Store == 20)
ggplot(store_graph,aes(x=CPI,y=Weekly_Sales)) + geom_point(aes(color=store_graph$IsHoliday)) + geom_smooth()
```
Finally, look for a correlation matrix between all of our numerical features.  
```{r,echo=FALSE}
#Correlation Graph
corrplot.mixed(cor(train.train[,c(-1,-2,-5,-17)]), lower = "ellipse",upper = "number")
```
#Modelling
Since we saw that the Store type is very important to predict the Weekly Sales of a given store, we will run a Decision Tree model to predict what Type a Store should be based on the 
different features that we have on our model.  
**Decision Tree**

```{r,Type}
#Using a decision tree we will like to predict the Type of a store based on all the other parameters
train.rpart <-rpart(Type ~ Weekly_Sales + Size,data=train.train, control=rpart.control(minsplit=1,cp=0.05))
summary(train.rpart)
fancyRpartPlot(train.rpart)
dim(train.train)
```
Now we will also like to form a Decision Tree for predicting the Rank that each store lies
with respect to their sales. We want to look at the other features in order to predict what range of sales a store will have in the future taking into account anything but previous sales. 
```{r,eval=FALSE,Rank}
rannk.rpart <- load("rank.Rdata")
head(train.train)
rank.rpart <- rpart(Rank~ .-Weekly_Sales-Size,data=train.train,control=rpart.control(minsplit=10,cp=0.03))
fancyRpartPlot(rank.rpart)
```
![Rank Decision Tree](/Users/sebastianalvarado/iXperience/Course/Week3/ModelProject/Walmart/RankDT.png)

**Linear Regression**
We would also like to create a linear model to find a specific value for Weekly Sales
that we want to predict. This line of best fit is intended to approximate further data points based on the line that we find in our training data. 
```{r}
fit <- lm(Weekly_Sales ~.-Date-Type-CPI, data=train.train)
predict_fit_confidence <- predict(fit, newdata=train.test, interval="confidence", level=0.95)
summary(fit)
```

#Model Evaluation and Results
First we will evaluate the result of our Type prediction Model in order to have a clearer
picture of its accuracy with unseen data.
```{r}
prediction <- predict(train.rpart,train.test, type="class")
train.test$Prediction <- prediction
#Find the percentage accuracy of our model
accur_table <- train.test %>% select(Type,Prediction) 
bool_vector <- accur_table$Type == accur_table$Prediction
length(which(bool_vector)) / length(bool_vector)
```
We can see that the accuracy is a high 97%, so we can conclude that a Decision Tree is 
a very powerful technique for this data set. Since the Type of the store is really significant 
as we saw in our exploration. This result can help the company categorize new stores and therefore predict how much they should sell based on the Type of store they are grouped into. 

Second, we evaluate the accuracy of our Rank prediction. This will help us to know within which range a certain store should sell in a given week of the year.
```{r,eval=FALSE}
#Evaluate Results of the Model
prediction_rank <- predict(rank.rpart,train.test, type="class")
train.test$RankPred <- prediction_rank

accuracy_test <- train.test %>% select(Rank,RankPred)
values <- accuracy_test$Rank == accuracy_test$RankPred
length(which(values)) / length(values)
```
```{r}

```
